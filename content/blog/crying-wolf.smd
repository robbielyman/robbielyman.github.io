---
.title = "So, is coding dead yet or what? My students want to know",
.date = @date("2026-02-13T14:30:00-05:00"),
.author = "Robbie Lyman",
.layout = "post.shtml",
.draft = false,
---

CEOs of AI startups have been claiming that software engineering is about 6 months away from total obsolescence more or less continuously since maybe 2023,
with [Anthropic's CEO the most recent to say so about three weeks ago](https://www.entrepreneur.com/business-news/ai-ceo-says-software-engineers-could-be-replaced-in-months/502087).
As in [Aesop's fable of the boy who cried wolf](https://en.wikipedia.org/wiki/The_Boy_Who_Cried_Wolf),
I think there are diminishing returns to making these claims again and again.
Even if or as LLM capabilities improve and the job of software engineering changes,
the doomsday speak is tiresome.

To be sure, LLM models have increased dramatically in their ability to code,
from laughably unable to do anything sensible in the pre-ChatGPT era
to new "agentic" workflows starting to see widespread adoption.
If you pattern-match, some of the stuff currently squarely in the "laughably unable to do anything sensible" camp,
like [Gas Town](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04),
_may_ stand to see significant improvement over a timeline best measured in years, not months.

But: let's cut the crying wolf routine.
I don't want to read any more blog posts about how coding is dead.

I would _love_ to read posts about how to focus on product engineering if 
the writing code step can be either mostly or completely outsourced to LLM agents.
What have you learned about what makes that work successful or difficult?
Are those skills complementary to being able to reason about code or orthogonal to it?
Unfortunately, every post I read is either predicting the end of the world—at least, jobs-wise—suffers from a worrying lack of a grip on reality, or is obviously just trying to sell me something.

Here's why it matters to me:
I'm a relative outsider—my employment work is at a university, not a tech company.
Most of my coding work goes into maintaining and building open source projects in Zig.
Since I'm interested as much in building my own knowledge as I am about the quality of the code I produce,
I'm more likely to implement a classical algorithm,
(for example constructing a Delaunay triangulation of a point cloud)
than to build a CRUD app with or without AI assistance.

However, as a teacher in a department of math and computer science,
I know that my students are entering a very different technical landscape than I did graduating college.
I'm sure that they need engineering skills that look a little different than being able to write depth-first search correctly,
and I would love to help convey those skills to them so they can succeed.

None of the three kinds of LLM booster writing I diagnose above is helpful to me in that goal.
In fact, little of it is good writing.

Folks in the first camp,
appear to want to step to, e.g., [Cory Doctorow](https://www.theguardian.com/us-news/ng-interactive/2026/jan/18/tech-ai-bubble-burst-reverse-centaur).
The point of these posts is to be right about the future.
That's certainly interesting, but parrotting the kind of hype performances that AI CEOs
put on for the benefit of their shareholders is not _particularly_ incisive insight,
if I may be so bold.

Folks in the Steve Yegge camp, whether they're right or not, are basically impossible to learn from.
Steve's article is compelling the way a train wreck is.
Imagine reading programming classics like [Crafting Interpreters](https://www.craftinginterpreters.com/)
or [Seven Languages in Seven Weeks](https://pragprog.com/titles/btlang/seven-languages-in-seven-weeks/)
but having to worry at the same time about the wellness of the author.
It's hard to build credibility if you type like you're halfway through a crazed 48-hour coding bender.

Then there's the selling.
Content works great as advertising!
I've been on Instagram, I know how it is.
That's not to say that it can't be educationally valuable at the same time,
but this kind of stuff is ultimately aimed to entertain, rather than to teach.
This kind of writing is all over the internet, of course, too,
so it's well-within ChatGPT's training data to generate more.

Now, I'm not saying you have to write a book—although please don't let me stop you!
But, if you're learning a lot using AI and are interested to be part of the public conversation as it shapes the future of coding, here's my suggestion to you:

Write what you've learned about your work, now that you're not nose-to-the-terminal in code all the time.
As you do so, write in the kind of measured, informative tone of voice you remember from writing that has inspired you.
You don't need to sound like a textbook,
but you do need to build trust, not provoke a reaction.
Above all, do not cry wolf. Do not pass Go, do not collect $200.
